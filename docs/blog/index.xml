<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Tabaré Capitán</title>
<link>https://www.tabarecapitan.com/blog/</link>
<atom:link href="https://www.tabarecapitan.com/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Projects and blog on causal inference.</description>
<generator>quarto-1.7.33</generator>
<lastBuildDate>Mon, 05 Jan 2026 23:00:00 GMT</lastBuildDate>
<item>
  <title>Introducing ritest: randomisation inference in Python</title>
  <link>https://www.tabarecapitan.com/blog/0002-ritest-intro/</link>
  <description><![CDATA[ 






<p>A few months ago I was analysing data from a randomised experiment aimed at increasing product adoption. It was the kind of project that shows up everywhere: a new feature ships, some users see it, some do not, and the goal is to figure out whether the feature had the intended effect.</p>
<p>The obvious next step is a <img src="https://latex.codecogs.com/png.latex?t">-test. That is what most analyses of this kind start with, and often where they stop.</p>
<p>But in this setting, the only thing that was actually random was the assignment itself: who saw the feature and who did not. The outcomes were not sampled at random from a population; they were observed after a deliberate assignment.</p>
<p>Instead of asking what would happen if I repeatedly sampled new users, I wanted to know what would have happened under different random assignments of the same users. This is the logic of randomisation inference.</p>
<p>I’ve done this before in Stata, where a well-established command, <code>ritest</code>, covers most practical uses of randomisation inference. But I was working in Python. I found tools that cover some uses, but I did not find a functional equivalent to Stata’s <code>ritest</code>.</p>
<p>So I wrote Python’s <code>ritest</code>.</p>
<p>This post is a short announcement. My new package, <a href="https://tabarecapitan.com/projects/ritest/"><code>ritest</code></a>, brings a familiar randomisation inference tool to Python. It is designed to be easy to use, flexible, and fast.</p>
<section id="randomisation-inference" class="level2">
<h2 class="anchored" data-anchor-id="randomisation-inference">Randomisation inference</h2>
<p>When an experiment is randomised, there are two different stories you can tell about uncertainty.</p>
<p>One story is the ‘sampling’ story. You imagine your dataset as one draw from a larger population, and you ask what would happen if you could repeat the data-collection process. That is the story behind most textbook standard errors and t-tests.</p>
<p>The other story is the ‘assignment’ story. You hold the outcomes fixed and ask what would have happened under different random assignments of the same treatment. That is the story behind randomisation inference.</p>
<p>Operationally, randomisation inference is simple:</p>
<ol type="1">
<li>pick a statistic that measures the effect you care about</li>
<li>compute it on the observed assignment</li>
<li>recompute it under many alternative assignments that respect the experimental design</li>
<li>compare the observed statistic to its randomisation distribution</li>
</ol>
<p>That’s it. The hard part, in practice, is doing it in a way that is fast enough to use, and strict enough about the design to be trustworthy.</p>
</section>
<section id="features" class="level2">
<h2 class="anchored" data-anchor-id="features">Features</h2>
<p><code>ritest</code> supports two ways of defining the test statistic. In the most common case, the statistic is a coefficient from a linear model, specified through a regression formula. When that is not appropriate, you can instead provide a custom Python function that maps the data to a single scalar statistic.</p>
<p>In both cases, permutations can be constrained to respect the experimental design, including stratified randomisation, clustered assignment, and optional weighting on the linear path.</p>
<p>By default, <code>ritest</code> makes the Monte Carlo uncertainty in the p-value explicit when permutations are sampled rather than enumerated (which is almost always true). In that case, the p-value itself is an estimate, and the output includes a confidence interval for that estimate. On the linear path, the package also reports coefficient bounds (or a confidence interval) by default.</p>
<p>The package can be installed from PyPI:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">pip install ritest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>python</span></code></pre></div>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p>Here is a realistic pattern from product work. Imagine a rollout where users are randomised to see a new onboarding flow. The outcome is whether the user activates within 7 days. You also have pre-treatment covariates that help with precision (previous activity, device type, country). The effect you want is the coefficient on <code>treat</code>.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> ritest <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ritest</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Example column meanings:</span></span>
<span id="cb2-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - activated_7d: 0/1 (activated within 7 days)</span></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - treat: 0/1 (assigned to new onboarding)</span></span>
<span id="cb2-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - pre_usage: numeric (pre-treatment engagement)</span></span>
<span id="cb2-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - device_ios: 0/1 (pre-built dummy; you can build dummies upstream)</span></span>
<span id="cb2-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - region_eu: 0/1 (pre-built dummy)</span></span>
<span id="cb2-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># - strata_id: str/int (block or bucket used in the randomisation)</span></span>
<span id="cb2-11"></span>
<span id="cb2-12">res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ritest(</span>
<span id="cb2-13">    df<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df,</span>
<span id="cb2-14">    permute_var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"treat"</span>,</span>
<span id="cb2-15">    formula<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"activated_7d ~ treat + pre_usage + device_ios + region_eu"</span>,</span>
<span id="cb2-16">    stat<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"treat"</span>,</span>
<span id="cb2-17">    strata<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"strata_id"</span>,</span>
<span id="cb2-18">    reps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9999</span>,</span>
<span id="cb2-19">    alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>,</span>
<span id="cb2-20">    seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">123</span>,</span>
<span id="cb2-21">)</span>
<span id="cb2-22"></span>
<span id="cb2-23"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(res.summary())</span></code></pre></div>
<p>This is the workflow I personally wanted: I can express the estimand as a familiar regression coefficient, and I can get assignment-based uncertainty without pretending the only randomness in the problem is sampling noise.</p>
<p>Now imagine that the adoption question is not your bottleneck. Your bottleneck is latency: you care about the median time-to-value, which is skewed and full of long tails. You still have a randomised assignment, but you do not want to force the problem into a linear model.</p>
<p>That is what the generic path is for.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> ritest <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ritest</span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> median_diff(d):</span>
<span id="cb3-4">    treated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d.loc[d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"treat"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"time_to_value_hours"</span>].median()</span>
<span id="cb3-5">    control <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d.loc[d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"treat"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"time_to_value_hours"</span>].median()</span>
<span id="cb3-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> treated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> control</span>
<span id="cb3-7"></span>
<span id="cb3-8">res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ritest(</span>
<span id="cb3-9">    df<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df,</span>
<span id="cb3-10">    permute_var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"treat"</span>,</span>
<span id="cb3-11">    stat_fn<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>median_diff,</span>
<span id="cb3-12">    reps<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9999</span>,</span>
<span id="cb3-13">    alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>,</span>
<span id="cb3-14">    seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">123</span>,</span>
<span id="cb3-15">)</span>
<span id="cb3-16"></span>
<span id="cb3-17"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(res.pvalue)</span></code></pre></div>
<p>The point is not that medians are “better” than conditional means (coefficients). The point is that a real workflow often has both kinds of questions, and the underlying source of uncertainty (the assignment) is the same.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>I built this package because I needed it. The project grew well beyond my original plan as I tried to emulate, in Python, the same sense of convenience I had relied on when doing randomisation inference in Stata. I’m happy with the result, and I hope others find it useful. Since this is my first time releasing a package on PyPI, I genuinely want to hear what people think.</p>
<p>Finally, I want to encourage data scientists, data analysts, and researchers who are not familiar with randomisation inference to take a closer look. Randomisation inference can be appropriate whenever assignment is controlled and known. This is a common setting in many contexts: A/B testing in product and platform experiments, randomised controlled trials in economics and political science, greenhouse and field experiments in agricultural science, and laboratory or clinical studies in life sciences. If the main source of uncertainty in your problem comes from the design itself, randomisation inference may be right for you.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a> ]]></description>
  <category>statistics</category>
  <category>software</category>
  <guid>https://www.tabarecapitan.com/blog/0002-ritest-intro/</guid>
  <pubDate>Mon, 05 Jan 2026 23:00:00 GMT</pubDate>
</item>
<item>
  <title>On inference</title>
  <link>https://www.tabarecapitan.com/blog/0001-inference/</link>
  <description><![CDATA[ 






<p>Consider the following hypothetical example. Spotify is investing in audiobooks, and wants to learn how much more discovery it can drive without harming core music listening. An obvious first step is an A/B test: add an ‘Audiobooks’ shelf to the Home feed for <em>some</em> eligible users. After a couple of weeks, estimate the treated minus control difference in time spent listening to books, with a guardrail like time spent listening to music.</p>
<p>If the assignment was random, that difference has a clean causal interpretation <em>for this experiment, for these users, over this window</em>. Identification is straightforward. Inference is more nuanced: how uncertain is the estimate of the difference, and uncertain <em>about what</em>? In other words, what can we infer about the world <em>beyond</em> this particular experiment?</p>
<p>This post is me trying to get the concept of inference straight. I’m going to treat ‘inference’ as a question about the <em>story of what could have happened</em>, not as a set of techniques I can apply.</p>
<section id="inference-is-a-thought-experiment" class="level2">
<h2 class="anchored" data-anchor-id="inference-is-a-thought-experiment">Inference is a thought experiment</h2>
<p>Inference is always a thought experiment. In our example, we get a point estimate for <em>that experiment, for those users, over that window</em>. What if we had another experiment? What if we had other users? What if we had another window? Unfortunately, that we cannot see. And so we rely on thought experiments.</p>
<p>Confidence intervals and <img src="https://latex.codecogs.com/png.latex?p">-values answer those ‘what if’ questions within a given thought experiment: What would we see if the world replayed repeatedly, in some relevant sense? That replay is not a minor detail. It is the <em>definition</em> of what your uncertainty statement means. And in most settings there are two replay modes that make immediate sense.</p>
<p><strong>Mode 1: replay the users</strong>. Imagine Spotify could re-run the same experiment many times, but each time the platform happens to see a different slice of users: different people are active, eligible, reachable, or simply online during your windows. You run the same A/B each time, and your estimate moves around <em>because the people changed</em>. This is sampling-based uncertainty.</p>
<p>That story corresponds to the <em>classical statistical inference</em> we typically encounter in textbooks and beyond: <img src="https://latex.codecogs.com/png.latex?p">-values motivated via repeated sampling. The key idea is that your effect (point estimate) could have been different had your sample of users been different. That is the uncertainty you are trying to estimate.</p>
<p><strong>Mode 2: replay the assignment</strong>. Now hold the users fixed. Imagine Spotify could take the same users, same window, and same (potential) outcomes. The only thing you replay is the randomisation: who got the ‘Audiobooks’ shelf and who didn’t, respecting whatever rules you originally used (such as equal split, stratification, blocked randomisation, and so on).</p>
<p>That story is the realm of <em>randomisation inference</em>. It is also the clean way to interpret permutation tests in an experiment: you are not permuting “because it is non-parametric”; you are generating the distribution of your statistic under the assignment mechanism you actually used.</p>
</section>
<section id="quantifying-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="quantifying-uncertainty">Quantifying uncertainty</h2>
<p>I find it useful to think about inference in three layers. The first layer is the estimator (or statistic): it produces an estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> of a target effect <img src="https://latex.codecogs.com/png.latex?%5Ctau">. You need <em>something</em> to make an inference about. Furthermore, random assignment lends causal credibility to the interpretation of the estimate. The second layer relates to the scope of the inference. Are we trying to make inferences about the broader population we want to generalise to? (replay mode 1) Or are we trying to make inferences about who happened to see the ‘Audiobooks’ shelf due to the particular realisation of the randomisation process? (replay mode 2) Or maybe both? The third layer refers to the quantification of the uncertainty <em>within the scope of the inference</em>. It is here that we can find the many methods that take the first two layers and turn them into <img src="https://latex.codecogs.com/png.latex?p">-values and confidence intervals.</p>
<p>For example, in our hypothetical experiment, the first layer is the estimator. We compute the treatment effect estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">, for instance as the OLS coefficient on the treatment indicator, which (with an intercept) is algebraically equal to the treated–control difference in means, <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Ctau%7D%20%5C;=%5C;%20%5Cbar%7BY%7D_%7BT%7D%20-%20%5Cbar%7BY%7D_%7BC%7D.%0A"></p>
<p>Suppose that, in the second layer, we adopt a sampling-based replay story: the estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> would have been different had the experiment observed a different random sample of users.</p>
<p>In the third layer, we can quantify that uncertainty in several ways; with its interpretation being contingent on the sampling-based story.</p>
<p>A <em>standard error</em> is an absolute measure of dispersion. It estimates the variability of the estimator across repeated samples, <img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)%20%5C;%5Capprox%5C;%20%5Csqrt%7B%5Coperatorname%7BVar%7D(%5Chat%7B%5Ctau%7D)%7D.%0A"></p>
<p>A <em>confidence interval</em> converts the same idea into an absolute uncertainty range for the estimand (the target effect). Under a Normal approximation, a <img src="https://latex.codecogs.com/png.latex?(1-%5Calpha)"> confidence interval for <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is <img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5B%0A%5Chat%7B%5Ctau%7D%20-%20z_%7B1-%5Calpha/2%7D%5C,%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D),%0A%5C;%5C;%0A%5Chat%7B%5Ctau%7D%20+%20z_%7B1-%5Calpha/2%7D%5C,%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)%0A%5Cright%5D,%0A"> where <img src="https://latex.codecogs.com/png.latex?z_%7B1-%5Calpha/2%7D"> denotes the corresponding quantile of the standard Normal distribution (or the appropriate <img src="https://latex.codecogs.com/png.latex?t"> quantile in finite samples).</p>
<p>A <em><img src="https://latex.codecogs.com/png.latex?p">-value</em> is different in nature: it is defined only relative to a hypothesis. If we wish to assess compatibility with a specific reference value <img src="https://latex.codecogs.com/png.latex?%5Ctau_0"> (often <img src="https://latex.codecogs.com/png.latex?%5Ctau_0%20=%200">), we form the standardised statistic <img src="https://latex.codecogs.com/png.latex?%0At%20%5C;=%5C;%20%5Cfrac%7B%5Chat%7B%5Ctau%7D%20-%20%5Ctau_0%7D%7B%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)%7D.%0A"></p>
<p>Under the sampling-based assumptions and the chosen reference distribution, the <img src="https://latex.codecogs.com/png.latex?p">-value is <img src="https://latex.codecogs.com/png.latex?%0Ap%20%5C;=%5C;%20%5CPr%5C!%5Cleft(%20%7CT%7C%20%5Cge%20%7Ct_%7B%5Ctext%7Bobs%7D%7D%7C%20%5C;%5Cmiddle%7C%5C;%20H_0:%5Ctau=%5Ctau_0%20%5Cright),%0A"> that is, the probability—computed under the null hypothesis—that a re-sampled experiment would produce a standardised statistic at least as extreme as the one observed.<sup>1</sup></p>
<p>So far, this may look like ‘methods’: <img src="https://latex.codecogs.com/png.latex?t">-tests, confidence intervals, p-values. But the three layers are the point. None of these outputs make sense in isolation. The meaning comes from (i) the estimator, (ii) the replay story, and only then (iii) the calculator used to turn the story into numbers.</p>
</section>
<section id="uncertainty-calculators" class="level2">
<h2 class="anchored" data-anchor-id="uncertainty-calculators">Uncertainty calculators</h2>
<p>Now that the estimator and the replay story are fixed, the remaining question is how to <em>compute</em> uncertainty within that scope. This is where most methods people recognise live. They are mostly different ways of approximating the same object: the distribution of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> under the chosen replay mode.</p>
<p>There are two broad ways to get that distribution.</p>
<p><strong>Route A: estimate variability, then approximate a reference distribution.</strong> This is the standard error route. You compute <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)">, form a standardised statistic, and then map it to a <img src="https://latex.codecogs.com/png.latex?p">-value (or CI) using a reference distribution (Normal or <img src="https://latex.codecogs.com/png.latex?t"> in simple cases). This family includes the classic <img src="https://latex.codecogs.com/png.latex?t">-test and its close relatives (Wald tests, <img src="https://latex.codecogs.com/png.latex?F"> tests, <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> tests), all of which share the same structure: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bstatistic%7D%20%5Cquad%20%5Crightarrow%20%5Cquad%20%5Ctext%7Bestimated%20variability%7D%20%5Cquad%20%5Crightarrow%20%5Cquad%20%5Ctext%7Breference%20distribution%7D.%0A"></p>
<p>Within this route, you still have choices about the variability estimate. In regression output, for instance, a <em>model-based</em> OLS standard error is tied to a particular noise model, while a <em>robust (sandwich)</em> standard error is designed to be less dependent on that model. The estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> can be identical, while the attached uncertainty calculation changes because the calculator changed. That is why Freedman’s (2008) warning matters even in experiments: randomisation can justify the causal meaning of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">, while leaving room for disagreement (or mistakes) about the standard error attached to it.</p>
<p><strong>Route B: build a reference distribution directly by replaying the world.</strong> This is the resampling or re-randomisation route. Instead of estimating an <img src="https://latex.codecogs.com/png.latex?SE"> and leaning on a Normal or <img src="https://latex.codecogs.com/png.latex?t"> approximation, you generate many ‘replays’ and recompute the statistic each time. The output is an empirical distribution of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">, from which you can read off uncertainty summaries.</p>
<p>Two big families sit here:</p>
<ul>
<li><p><strong>Bootstrap and jackknife (sampling replay):</strong> you replay <em>which users you observed</em> by resampling units (bootstrap) or systematically leaving them out (jackknife). You can then compute <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)"> as the standard deviation of the replicated estimates, or compute confidence intervals from quantiles of the empirical distribution. A <img src="https://latex.codecogs.com/png.latex?p">-value is also possible, but it requires an explicit hypothesis construction, just like before.</p></li>
<li><p><strong>Randomisation inference (assignment replay):</strong> you replay <em>who was treated</em> by re-running the randomisation procedure many times, respecting the original design. Under a sharp null, this directly gives a reference distribution for your statistic under the assignment mechanism.<sup>2</sup> A <img src="https://latex.codecogs.com/png.latex?p">-value can be computed with the most literal tail probability: <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bp%7D%20=%5C;%20%5Cfrac%7B%5C#%5C%7B%7CT_r%7C%20%5Cge%20%7CT_%7B%5Ctext%7Bobs%7D%7D%7C%5C%7D%7D%7BR%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?R"> is the number of simulated reassignments and <img src="https://latex.codecogs.com/png.latex?T_r"> is the statistic under reassignment <img src="https://latex.codecogs.com/png.latex?r">. Notice what is missing: there is no required step of ‘estimate an <img src="https://latex.codecogs.com/png.latex?SE"> and assume Normality’. The design supplies the reference distribution.<sup>3</sup></p></li>
</ul>
</section>
<section id="practical-implications" class="level2">
<h2 class="anchored" data-anchor-id="practical-implications">Practical implications</h2>
<p>You may have felt something off up to this point. In our Spotify example, the effect estimate is justified by random assignment (design logic), while a lot of standard inference is presented through a sampling lens. Furthermore, in plain A/B tests, you often find that robust <img src="https://latex.codecogs.com/png.latex?t">-tests, bootstrap uncertainty, and randomisation-based checks all tell the same story.</p>
<p>This is not because the layers collapse into one. It is because the situation is unusually ‘friendly’:</p>
<ul>
<li><strong>The estimator is simple.</strong> A difference in means is a stable object.</li>
<li><strong>Sample sizes are large.</strong> Many distributions become well-behaved once you have enough users, and many reasonable standardisations start to look similar.</li>
<li><strong>Different variance calculators converge.</strong> In the binary-treatment case, several common standard-error formulas are built from the same ingredients (treated and control variability and group sizes), so their numerical differences can get washed out.</li>
</ul>
<p>If all you need is a quick answer to ‘did the shelf move audiobook listening?’, this is why your preferred software’s default often feels like it ‘just works’.</p>
<p>But the friendly zone is not guaranteed. The moment the design stops being “randomise users 50/50”, the replay world changes, and the calculator has to match it.</p>
<p>For example, if you randomised within strata (country, device, prior engagement), then ‘replay the assignment’ means reshuffling <em>within strata</em>. A calculator that ignores this is quantifying uncertainty for a world that never could have happened. Alternatively, if assignment happens at a higher level (households, classrooms, markets), the effective sample size is the number of clusters, not the number of users. Many default approximations become fragile when there are few clusters.</p>
<p>This is the practical take of the three layers: inference is not a button you press after you get <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">. It is the combination of (i) what you estimated, (ii) what you think could have happened, and (iii) how you chose to quantify that.</p>
</section>
<section id="reading-list" class="level2">
<h2 class="anchored" data-anchor-id="reading-list">Reading list</h2>
<p>The references below have been helpful to me; they are not in any way meant as a comprehensive survey.</p>
<p>⭐ Abadie, A., Athey, S., Imbens, G., and Wooldridge, J. 2020. “Sampling-Based versus Design-Based Uncertainty in Regression Analysis.” <em>Econometrica</em>. <a href="https://economics.mit.edu/sites/default/files/publications/ECTA12675.pdf">link</a></p>
<p>Athey, S., &amp; Imbens, G. W. (2017). The econometrics of randomized experiments. In <em>Handbook of economic field experiments</em>. North-Holland. <a href="https://arxiv.org/abs/1607.00698">link</a></p>
<p>Freedman, David A. 2008. “On Regression Adjustments to Experimental Data.” <em>Advances in Applied Mathematics</em>. <a href="https://www.stat.berkeley.edu/~census/neyregr.pdf">link</a></p>
<p>Imbens, G. W., &amp; Rubin, D. B. (2015). <em>Causal inference in statistics, social, and biomedical sciences</em>. Cambridge university press. <a href="https://books.google.se/books?id=Bf1tBwAAQBAJ">link</a></p>
<p>Spotify. 2025-03-13. “How Spotify Is Driving Growth, Discovery, and Innovation in the Audiobook Market.” <em>Spotify Newsroom.</em> <a href="https://newsroom.spotify.com/2025-03-13/how-spotify-is-driving-growth-discovery-and-innovation-in-the-audiobook-market/">link</a></p>


</section>


<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Note that inference can exist without hypothesis testing; testing is a decision layered on top of an uncertainty statement, not its foundation.↩︎</p></li>
<li id="fn2"><p>Without additional structure, this exactness is tied to sharp nulls; a null about an average effect does not by itself pin down the missing potential outcomes. You can read more about randomisation inference with weak nulls (such as related to ATE) in <a href="https://arxiv.org/abs/1809.07419">this paper</a> published <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1750415?casa_token=hMXx_Nl6vIMAAAAA:DMVLAKErKTsFGYb898Sr5wHC2Uxtt2_JIJ-ATIsahRreYayFZ_VaYPF6Q6dJpXsNc8newgsWgp-Bgg">in JASA</a>.↩︎</p></li>
<li id="fn3"><p>If you approximate a randomisation <img src="https://latex.codecogs.com/png.latex?p">-value by simulation, then <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D"> has Monte Carlo error because <img src="https://latex.codecogs.com/png.latex?R"> is finite. If <img src="https://latex.codecogs.com/png.latex?c"> is the number of simulated statistics at least as extreme as <img src="https://latex.codecogs.com/png.latex?T_%7B%5Ctext%7Bobs%7D%7D">, then <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D=c/R">. Treating <img src="https://latex.codecogs.com/png.latex?c%20%5Csim%20%5Coperatorname%7BBinomial%7D(R,p)"> gives a simple way to compute a confidence interval for <img src="https://latex.codecogs.com/png.latex?p"> (for example via a Clopper–Pearson or Wilson interval). This is uncertainty about the Monte Carlo approximation, not uncertainty about the treatment effect.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>statistics</category>
  <guid>https://www.tabarecapitan.com/blog/0001-inference/</guid>
  <pubDate>Sun, 28 Dec 2025 23:00:00 GMT</pubDate>
</item>
</channel>
</rss>
