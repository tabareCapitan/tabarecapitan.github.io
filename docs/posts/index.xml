<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Tabaré Capitán</title>
<link>https://www.tabarecapitan.com/posts/</link>
<atom:link href="https://www.tabarecapitan.com/posts/index.xml" rel="self" type="application/rss+xml"/>
<description>Data scientist, personal website</description>
<generator>quarto-1.7.33</generator>
<lastBuildDate>Sun, 28 Dec 2025 23:00:00 GMT</lastBuildDate>
<item>
  <title>On inference</title>
  <link>https://www.tabarecapitan.com/posts/0001-inference/</link>
  <description><![CDATA[ 






<p>Consider the following hypothetical example. Spotify is investing in audiobooks, and wants to learn how much more discovery it can drive without harming core music listening. An obvious first step is an A/B test: add an ‘Audiobooks’ shelf to the Home feed for <em>some</em> eligible users. After a couple of weeks, estimate the treated minus control difference in time spent listening to books, with a guardrail like time spent listening to music.</p>
<p>If the assignment was random, that difference has a clean causal interpretation <em>for this experiment, for these users, over this window</em>. Identification is straightforward. Inference is more nuanced: how uncertain is the estimate of the difference, and uncertain <em>about what</em>? In other words, what can we infer about the world <em>beyond</em> this particular experiment?</p>
<p>This post is me trying to get the concept of inference straight. I’m going to treat ‘inference’ as a question about the <em>story of what could have happened</em>, not as a set of techniques I can apply.</p>
<section id="inference-is-a-thought-experiment" class="level2">
<h2 class="anchored" data-anchor-id="inference-is-a-thought-experiment">Inference is a thought experiment</h2>
<p>Inference is always a thought experiment. In our example, we get a point estimate for <em>that experiment, for those users, over that window</em>. What if we had another experiment? What if we had other users? What if we had another window? Unfortunately, that we cannot see. And so we rely on thought experiments.</p>
<p>Confidence intervals and <img src="https://latex.codecogs.com/png.latex?p">-values answer those ‘what if’ questions within a given thought experiment: What would we see if the world replayed repeatedly, in some relevant sense? That replay is not a minor detail. It is the <em>definition</em> of what your uncertainty statement means. And in most settings there are two replay modes that make immediate sense.</p>
<p><strong>Mode 1: replay the users</strong>. Imagine Spotify could re-run the same experiment many times, but each time the platform happens to see a different slice of users: different people are active, eligible, reachable, or simply online during your windows. You run the same A/B each time, and your estimate moves around <em>because the people changed</em>. This is sampling-based uncertainty.</p>
<p>That story corresponds to the <em>classical statistical inference</em> we typically encounter in textbooks and beyond: <img src="https://latex.codecogs.com/png.latex?p">-values motivated via repeated sampling. The key idea is that your effect (point estimate) could have been different had your sample of users been different. That is the uncertainty you are trying to estimate.</p>
<p><strong>Mode 2: replay the assignment</strong>. Now hold the users fixed. Imagine Spotify could take the same users, same window, and same (potential) outcomes. The only thing you replay is the randomisation: who got the ‘Audiobooks’ shelf and who didn’t, respecting whatever rules you originally used (such as equal split, stratification, blocked randomisation, and so on).</p>
<p>That story is the realm of <em>randomisation inference</em>. It is also the clean way to interpret permutation tests in an experiment: you are not permuting “because it is non-parametric”; you are generating the distribution of your statistic under the assignment mechanism you actually used.</p>
</section>
<section id="quantifying-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="quantifying-uncertainty">Quantifying uncertainty</h2>
<p>I find it useful to think about inference in three layers. The first layer is the estimator (or statistic): it produces an estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> of a target effect <img src="https://latex.codecogs.com/png.latex?%5Ctau">. You need <em>something</em> to make an inference about. Furthermore, random assignment lends causal credibility to the interpretation of the estimate. The second layer relates to the scope of the inference. Are we trying to make inferences about the broader population we want to generalise to? (replay mode 1) Or are we trying to make inferences about who happened to see the ‘Audiobook’ shelf due to the particular realisation of the randomisation process? (replay mode 2) Or maybe both? The third layer refers to the quantification of the uncertainty <em>within the scope of the inference</em>. It is here that we can find the many methods that take the first two layers and turn them into <img src="https://latex.codecogs.com/png.latex?p">-values and confidence intervals.</p>
<p>For example, in our hypothetical experiment, the first layer is the estimator. We compute the treatment effect estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">, for instance as the OLS coefficient on the treatment indicator, which (with an intercept) is algebraically equal to the treated–control difference in means, <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Ctau%7D%20%5C;=%5C;%20%5Cbar%7BY%7D_%7BT%7D%20-%20%5Cbar%7BY%7D_%7BC%7D.%0A"></p>
<p>Suppose that, in the second layer, we adopt a sampling-based replay story: the estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> would have been different had the experiment observed a different random sample of users.</p>
<p>In the third layer, we can quantify that uncertainty in several ways; with its interpretation being contingent on the sampling-based story.</p>
<p>A <em>standard error</em> is an absolute measure of dispersion. It estimates the variability of the estimator across repeated samples, <img src="https://latex.codecogs.com/png.latex?%0A%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)%20%5C;%5Capprox%5C;%20%5Csqrt%7B%5Coperatorname%7BVar%7D(%5Chat%7B%5Ctau%7D)%7D.%0A"></p>
<p>A <em>confidence interval</em> converts the same idea into an absolute uncertainty range for the estimand (the target effect). Under a Normal approximation, a <img src="https://latex.codecogs.com/png.latex?(1-%5Calpha)"> confidence interval for <img src="https://latex.codecogs.com/png.latex?%5Ctau"> is <img src="https://latex.codecogs.com/png.latex?%0A%5Cleft%5B%0A%5Chat%7B%5Ctau%7D%20-%20z_%7B1-%5Calpha/2%7D%5C,%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D),%0A%5C;%5C;%0A%5Chat%7B%5Ctau%7D%20+%20z_%7B1-%5Calpha/2%7D%5C,%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)%0A%5Cright%5D,%0A"> where <img src="https://latex.codecogs.com/png.latex?z_%7B1-%5Calpha/2%7D"> denotes the corresponding quantile of the standard Normal distribution (or the appropriate <img src="https://latex.codecogs.com/png.latex?t"> quantile in finite samples).</p>
<p>A <em><img src="https://latex.codecogs.com/png.latex?p">-value</em> is different in nature: it is defined only relative to a hypothesis. If we wish to assess compatibility with a specific reference value <img src="https://latex.codecogs.com/png.latex?%5Ctau_0"> (often <img src="https://latex.codecogs.com/png.latex?%5Ctau_0%20=%200">), we form the standardised statistic <img src="https://latex.codecogs.com/png.latex?%0At%20%5C;=%5C;%20%5Cfrac%7B%5Chat%7B%5Ctau%7D%20-%20%5Ctau_0%7D%7B%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)%7D.%0A"></p>
<p>Under the sampling-based assumptions and the chosen reference distribution, the <img src="https://latex.codecogs.com/png.latex?p">-value is <img src="https://latex.codecogs.com/png.latex?%0Ap%20%5C;=%5C;%20%5CPr%5C!%5Cleft(%20%7CT%7C%20%5Cge%20%7Ct_%7B%5Ctext%7Bobs%7D%7D%7C%20%5C;%5Cmiddle%7C%5C;%20H_0:%5Ctau=%5Ctau_0%20%5Cright),%0A"> that is, the probability—computed under the null hypothesis—that a re-sampled experiment would produce a standardised statistic at least as extreme as the one observed.<sup>1</sup></p>
<p>So far, this may look like ‘methods’: <img src="https://latex.codecogs.com/png.latex?t">-tests, confidence intervals, p-values. But the three layers are the point. None of these outputs make sense in isolation. The meaning comes from (i) the estimator, (ii) the replay story, and only then (iii) the calculator used to turn the story into numbers.</p>
</section>
<section id="uncertainty-calculators" class="level2">
<h2 class="anchored" data-anchor-id="uncertainty-calculators">Uncertainty calculators</h2>
<p>Now that the estimator and the replay story are fixed, the remaining question is how to <em>compute</em> uncertainty within that scope. This is where most methods people recognise live. They are mostly different ways of approximating the same object: the distribution of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> under the chosen replay mode.</p>
<p>There are two broad ways to get that distribution.</p>
<p><strong>Route A: estimate variability, then approximate a reference distribution.</strong><br>
This is the standard error route. You compute <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)">, form a standardised statistic, and then map it to a <img src="https://latex.codecogs.com/png.latex?p">-value (or CI) using a reference distribution (Normal or <img src="https://latex.codecogs.com/png.latex?t"> in simple cases). This family includes the classic <img src="https://latex.codecogs.com/png.latex?t">-test and its close relatives (Wald tests, <img src="https://latex.codecogs.com/png.latex?F"> tests, <img src="https://latex.codecogs.com/png.latex?%5Cchi%5E2"> tests), all of which share the same structure: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bstatistic%7D%20%5Cquad%20%5Crightarrow%20%5Cquad%20%5Ctext%7Bestimated%20variability%7D%20%5Cquad%20%5Crightarrow%20%5Cquad%20%5Ctext%7Breference%20distribution%7D.%0A"></p>
<p>Within this route, you still have choices about the variability estimate. In regression output, for instance, a <em>model-based</em> OLS standard error is tied to a particular noise model, while a <em>robust (sandwich)</em> standard error is designed to be less dependent on that model. The estimator <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> can be identical, while the attached uncertainty calculation changes because the calculator changed. That is why Freedman’s (2008) warning matters even in experiments: randomisation can justify the causal meaning of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">, while leaving room for disagreement (or mistakes) about the standard error attached to it.</p>
<p><strong>Route B: build a reference distribution directly by replaying the world.</strong><br>
This is the resampling or re-randomisation route. Instead of estimating an <img src="https://latex.codecogs.com/png.latex?SE"> and leaning on a Normal or <img src="https://latex.codecogs.com/png.latex?t"> approximation, you generate many ‘replays’ and recompute the statistic each time. The output is an empirical distribution of <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">, from which you can read off uncertainty summaries.</p>
<p>Two big families sit here:</p>
<ul>
<li><p><strong>Bootstrap and jackknife (sampling replay):</strong> you replay <em>which users you observed</em> by resampling units (bootstrap) or systematically leaving them out (jackknife). You can then compute <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BSE%7D(%5Chat%7B%5Ctau%7D)"> as the standard deviation of the replicated estimates, or compute confidence intervals from quantiles of the empirical distribution. A <img src="https://latex.codecogs.com/png.latex?p">-value is also possible, but it requires an explicit hypothesis construction, just like before.</p></li>
<li><p><strong>Randomisation inference (assignment replay):</strong> you replay <em>who was treated</em> by re-running the randomisation procedure many times, respecting the original design. Under a sharp null, this directly gives a reference distribution for your statistic under the assignment mechanism.<sup>2</sup> A <img src="https://latex.codecogs.com/png.latex?p">-value can be computed with the most literal tail probability: <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bp%7D%20=%5C;%20%5Cfrac%7B%5C#%5C%7B%7CT_r%7C%20%5Cge%20%7CT_%7B%5Ctext%7Bobs%7D%7D%7C%5C%7D%7D%7BR%7D,%0A"> where <img src="https://latex.codecogs.com/png.latex?R"> is the number of simulated reassignments and <img src="https://latex.codecogs.com/png.latex?T_r"> is the statistic under reassignment <img src="https://latex.codecogs.com/png.latex?r">. Notice what is missing: there is no required step of ‘estimate an <img src="https://latex.codecogs.com/png.latex?SE"> and assume Normality’. The design supplies the reference distribution.<sup>3</sup></p></li>
</ul>
</section>
<section id="practical-implications" class="level2">
<h2 class="anchored" data-anchor-id="practical-implications">Practical implications</h2>
<p>You may have felt something off up to this point. In our Spotify example, the effect estimate is justified by random assignment (design logic), while a lot of standard inference is presented through a sampling lens. Furthermore, in plain A/B tests, you often find that robust <img src="https://latex.codecogs.com/png.latex?t">-tests, bootstrap uncertainty, and randomisation-based checks all tell the same story.</p>
<p>This is not because the layers collapse into one. It is because the situation is unusually ‘friendly’:</p>
<ul>
<li><strong>The estimator is simple.</strong> A difference in means is a stable object.</li>
<li><strong>Sample sizes are large.</strong> Many distributions become well-behaved once you have enough users, and many reasonable standardisations start to look similar.</li>
<li><strong>Different variance calculators converge.</strong> In the binary-treatment case, several common standard-error formulas are built from the same ingredients (treated and control variability and group sizes), so their numerical differences can get washed out.</li>
</ul>
<p>If all you need is a quick answer to ‘did the shelf move audiobook listening?’, this is why your preferred software’s default often feels like it ‘just works’.</p>
<p>But the friendly zone is not guaranteed. The moment the design stops ‘being randomise users 50/50’, the replay world changes, and the calculator has to match it.</p>
<p>For example, if you randomised within strata (country, device, prior engagement), then ‘replay the assignment’ means reshuffling <em>within strata</em>. A calculator that ignores this is quantifying uncertainty for a world that never could have happened. Alternatively, if assignment happens at a higher level (households, classrooms, markets), the effective sample size is the number of clusters, not the number of users. Many default approximations become fragile when there are few clusters.</p>
<p>This is the practical take of the three layers: inference is not a button you press after you get <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">. It is the combination of (i) what you estimated, (ii) what you think could have happened, and (iii) how you chose to quantify that.</p>
</section>
<section id="reading-list" class="level2">
<h2 class="anchored" data-anchor-id="reading-list">Reading list</h2>
<p>The references below have been helpful to me; they are not in any way meant as a comprehensive survey.</p>
<p>⭐ Abadie, A., Athey, S., Imbens, G., and Wooldridge, J. 2020. “Sampling-Based versus Design-Based Uncertainty in Regression Analysis.” <em>Econometrica</em>. <a href="https://economics.mit.edu/sites/default/files/publications/ECTA12675.pdf">link</a></p>
<p>Athey, S., &amp; Imbens, G. W. (2017). The econometrics of randomized experiments. In <em>Handbook of economic field experiments</em>. North-Holland. <a href="https://arxiv.org/abs/1607.00698">link</a></p>
<p>Freedman, David A. 2008. “On Regression Adjustments to Experimental Data.” <em>Advances in Applied Mathematics</em>. <a href="https://www.stat.berkeley.edu/~census/neyregr.pdf">link</a></p>
<p>Imbens, G. W., &amp; Rubin, D. B. (2015). <em>Causal inference in statistics, social, and biomedical sciences</em>. Cambridge university press. <a href="https://books.google.se/books?id=Bf1tBwAAQBAJ">link</a></p>
<p>Spotify. 2025-03-13. “How Spotify Is Driving Growth, Discovery, and Innovation in the Audiobook Market.” <em>Spotify Newsroom.</em> <a href="https://newsroom.spotify.com/2025-03-13/how-spotify-is-driving-growth-discovery-and-innovation-in-the-audiobook-market/">link</a></p>


</section>


<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Note that inference can exist without hypothesis testing; testing is a decision layered on top of an uncertainty statement, not its foundation.↩︎</p></li>
<li id="fn2"><p>Without additional structure, this exactness is tied to sharp nulls; a null about an average effect does not by itself pin down the missing potential outcomes. You can read more about randomisation inference with weak nulls (such as related to ATE) in <a href="https://arxiv.org/abs/1809.07419">this paper</a> published <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2020.1750415?casa_token=hMXx_Nl6vIMAAAAA:DMVLAKErKTsFGYb898Sr5wHC2Uxtt2_JIJ-ATIsahRreYayFZ_VaYPF6Q6dJpXsNc8newgsWgp-Bgg">in JASA</a>.↩︎</p></li>
<li id="fn3"><p>If you approximate a randomisation <img src="https://latex.codecogs.com/png.latex?p">-value by simulation, then <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D"> has Monte Carlo error because <img src="https://latex.codecogs.com/png.latex?R"> is finite. If <img src="https://latex.codecogs.com/png.latex?c"> is the number of simulated statistics at least as extreme as <img src="https://latex.codecogs.com/png.latex?T_%7B%5Ctext%7Bobs%7D%7D">, then <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D=c/R">. Treating <img src="https://latex.codecogs.com/png.latex?c%20%5Csim%20%5Coperatorname%7BBinomial%7D(R,p)"> gives a simple way to compute a confidence interval for <img src="https://latex.codecogs.com/png.latex?p"> (for example via a Clopper–Pearson or Wilson interval). This is uncertainty about the Monte Carlo approximation, not uncertainty about the treatment effect.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>statistics</category>
  <guid>https://www.tabarecapitan.com/posts/0001-inference/</guid>
  <pubDate>Sun, 28 Dec 2025 23:00:00 GMT</pubDate>
</item>
</channel>
</rss>
